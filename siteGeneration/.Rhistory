templateIndex <- "templateIndex.md"
searchCodePage <- "searchTemplate.html"
stcntyFile <- paste0(dataDir, "MARISAstateFIPS.csv")
statefips = read.csv(paste0(dataDir, "stateFIPS.csv"))
##line break character
el <- "\n"
tab <- "  "
##read in the tool description document
toolInventoryFile <- list.files(dataDir, toolInventoryName, recursive=T, full.names=T)
toolInventoryFile <- toolInventoryFile[grep("~",toolInventoryFile, invert=TRUE)]
#toolInventoryFile <- descFile[grep(paste0("/",toolInventoryFile), descFile)]
##get number of sheets, to load all of them
sheetNames <- excel_sheets(toolInventoryFile)
##read in data
# Each list represents a seperate excel sheet
# toolInventory <- lapply(sheetNames, read_excel, path=toolInventoryFile, col_names=F)
# Use read.xlsx to get the merged cells
toolInventory <- lapply(sheetNames, read.xlsx, xlsxFile=toolInventoryFile, colNames=F, fillMergedCells = TRUE)
names(toolInventory) <- sheetNames
stcntyTab <- read.csv(stcntyFile) # Read the fips csv file
##inventory, cleaning headers to be useful
inventory <- toolInventory$Inventory # toolInventory[[1]]
inventory <- toolInventory$Inventory
catNames <- as.character(inventory[1,]) # Top line header (category names) for sheet 1: Inventory
baseNames <- as.character(inventory[2,]) # Secondary header (sub-category names) for sheet 1: Inventory
level3Names <- as.character(inventory[3,]) # Secondary header (sub-category names) for sheet 1: Inventory
level4Names <- as.character(inventory[4,]) # Secondary header (sub-category names) for sheet 1: Inventory
# Combine the top line and secondary headers to set as the column names
headers = rep(NA, length(catNames))
for(i in 1:length(catNames)){
if(is.na(catNames[i])){
headers[i] = baseNames[i]
} else {
headers[i] = paste(catNames[i], baseNames[i], sep="-")
}
}
# Find the duplicate headers and add the third category of names to the header
n_occur <- data.frame(table(headers))
dup_headers <- unique(headers[headers %in% n_occur$headers[n_occur$Freq > 1]])
ind_dup = unlist(lapply(X = dup_headers, function(X){which(headers == X)}))
headers[ind_dup] = paste(headers[ind_dup], level3Names[ind_dup], sep="-")
headers
n_occur_4level <- data.frame(table(headers))
dup_headers_4level <- unique(headers[headers %in% n_occur_4level$headers[n_occur_4level$Freq > 1]])
ind_dup_4level = unlist(lapply(X = dup_headers_4level, function(X){which(headers == X)}))
headers[ind_dup_4level] = paste(headers[ind_dup_4level], level4Names[ind_dup_4level], sep="-")
headers
ind_dup_4level
dup_headers_4level
"Description-Tags-Website friendliness" %in% dup_headers_4level
!("Description-Tags-Website friendliness" %in% dup_headers_4level)
# Combine the top line and secondary headers to set as the column names
headers = rep(NA, length(catNames))
for(i in 1:length(catNames)){
if(is.na(catNames[i])){
headers[i] = baseNames[i]
} else {
headers[i] = paste(catNames[i], baseNames[i], sep="-")
}
}
# Find the duplicate headers and add the third category of names to the header
n_occur <- data.frame(table(headers))
dup_headers <- unique(headers[headers %in% n_occur$headers[n_occur$Freq > 1]])
ind_dup = unlist(lapply(X = dup_headers, function(X){which(headers == X)}))
headers[ind_dup] = paste(headers[ind_dup], level3Names[ind_dup], sep="-")
# Find the duplicate headers and add the third category of names to the header
n_occur_4level <- data.frame(table(headers))
dup_headers_4level <- unique(headers[headers %in% n_occur_4level$headers[n_occur_4level$Freq > 1]])
# Make sure website friendliness is included (it is currently not a duplicate)
if(!("Description-Tags-Website friendliness" %in% dup_headers_4level)){
dup_headers_4level <- c(dup_headers_4level, "Description-Tags-Website friendliness")
}
ind_dup_4level = unlist(lapply(X = dup_headers_4level, function(X){which(headers == X)}))
headers[ind_dup_4level] = paste(headers[ind_dup_4level], level4Names[ind_dup_4level], sep="-")
headers
##libraries
library(readxl)
library(pbapply)
library(stringi)
# install.packages("webshot")
library(webshot)
library(openxlsx)
##webshot::install_phantomjs()
library(dplyr)
library(stringr)
#/Volumes/GoogleDrive/Shared drives/MARISA/Coastal Climate Extension Pilot/Tool Inventory/Tool Screenshots_resized_156_250/TOOLID_1.0_ScreenCapture-1.png
##find data file, assumed to be on computer
# baseDir <- "/Users/mdl5548/Documents/GitHub/"
# dataDir <- paste0(baseDir, "climToolsReference/siteGeneration/")
# #siteDir <- paste0(dataDir, "docs/")
# siteDir <- paste0(baseDir, "climToolsReference/content/")
# toolsDir <- paste0(siteDir, "tools_test/")
# discovDir <- paste0(baseDir, "climToolsReference/public/data_test/")
# imageDir <- paste0(baseDir, "climToolsReference/public/images_test/")
# toolSearchDir <- paste0(baseDir, "climToolsReference/themes/soho/layouts/page/")
baseDir <- "~/Documents/Github/"
dataDir <- paste0(baseDir, "climToolsReference/siteGeneration/")
source(paste0(dataDir, "siteGeneratingFunctions_2022.R"))
siteDir <- paste0(baseDir, "climToolsReference/content/")
toolsDir <- paste0(siteDir, "tools_test/")
discovDir <- paste0(baseDir, "climToolsReference/public/data_test/")
imageDir <- paste0(baseDir, "climToolsReference/public/images_test/")
#toolInventoryName <- "Inventory_2020-01-21_v2.xlsx"
toolInventoryName <- "Tool Inventory - 12.15.21 copy.xlsx"
templatePageName <- "pageTemplate.md"
templateIndex <- "templateIndex.md"
searchCodePage <- "searchTemplate.html"
stcntyFile <- paste0(dataDir, "MARISAstateFIPS.csv")
statefips = read.csv(paste0(dataDir, "stateFIPS.csv"))
##line break character
el <- "\n"
tab <- "  "
##read in the tool description document
toolInventoryFile <- list.files(dataDir, toolInventoryName, recursive=T, full.names=T)
toolInventoryFile <- toolInventoryFile[grep("~",toolInventoryFile, invert=TRUE)]
#toolInventoryFile <- descFile[grep(paste0("/",toolInventoryFile), descFile)]
##get number of sheets, to load all of them
sheetNames <- excel_sheets(toolInventoryFile)
##read in data
# Each list represents a seperate excel sheet
# toolInventory <- lapply(sheetNames, read_excel, path=toolInventoryFile, col_names=F)
# Use read.xlsx to get the merged cells
toolInventory <- lapply(sheetNames, read.xlsx, xlsxFile=toolInventoryFile, colNames=F, fillMergedCells = TRUE)
names(toolInventory) <- sheetNames
stcntyTab <- read.csv(stcntyFile) # Read the fips csv file
##inventory, cleaning headers to be useful
inventory <- toolInventory$Inventory # toolInventory[[1]]
inventory <- toolInventory$Inventory
catNames <- as.character(inventory[1,]) # Top line header (category names) for sheet 1: Inventory
baseNames <- as.character(inventory[2,]) # Secondary header (sub-category names) for sheet 1: Inventory
level3Names <- as.character(inventory[3,]) # Secondary header (sub-category names) for sheet 1: Inventory
level4Names <- as.character(inventory[4,]) # Secondary header (sub-category names) for sheet 1: Inventory
# Combine the top line and secondary headers to set as the column names
headers = rep(NA, length(catNames))
for(i in 1:length(catNames)){
if(is.na(catNames[i])){
headers[i] = baseNames[i]
} else {
headers[i] = paste(catNames[i], baseNames[i], sep="-")
}
}
# Find the duplicate headers and add the third category of names to the header
n_occur <- data.frame(table(headers))
dup_headers <- unique(headers[headers %in% n_occur$headers[n_occur$Freq > 1]])
ind_dup = unlist(lapply(X = dup_headers, function(X){which(headers == X)}))
headers[ind_dup] = paste(headers[ind_dup], level3Names[ind_dup], sep="-")
# Find the duplicate headers and add the third category of names to the header
n_occur_4level <- data.frame(table(headers))
dup_headers_4level <- unique(headers[headers %in% n_occur_4level$headers[n_occur_4level$Freq > 1]])
# Make sure website friendliness is included (it is currently not a duplicate)
if(!("Description-Tags-Website friendliness" %in% dup_headers_4level)){
dup_headers_4level <- c(dup_headers_4level, "Description-Tags-Website friendliness")
}
ind_dup_4level = unlist(lapply(X = dup_headers_4level, function(X){which(headers == X)}))
headers[ind_dup_4level] = paste(headers[ind_dup_4level], level4Names[ind_dup_4level], sep="-")
# # Combine the top line and secondary headers to set as the column names
# ##for now, hard coded
# baseNames[6:10] <- paste(catNames[6], baseNames[6:10], sep="-") # Description
# baseNames[11:15] <- paste(catNames[11], baseNames[11:15], sep="-") # Geographic Scope
# baseNames[16:24] <- paste(catNames[16], baseNames[16], as.character(inventory[3,][16:24]), sep="-") # Purpose - Tool Function
# baseNames[25:28] <- paste(catNames[16], baseNames[25], as.character(inventory[3,][25:28]), sep="-") # Purpose - Future Projections
# baseNames[29] <- paste(catNames[16], baseNames[29], sep="-") # Purpose - Description
# baseNames[30:34] <- paste(catNames[30], baseNames[30:34], sep="-") # Tool type
# baseNames[35:36] <- paste(catNames[35], baseNames[35:36], sep="-") # Tool Inputs
# baseNames[37:39] <- paste(catNames[37], baseNames[37:39], sep="-") # Tool Outputs
# baseNames[45] <- paste(catNames[37], baseNames[45], sep="-")# Tool Outputs
# baseNames[46:50] <- paste(catNames[46], baseNames[46:50], sep="-") # Software Requirements
# baseNames[51:53] <- paste(catNames[51], baseNames[51:53], sep="-") # Effort / Skill
# baseNames[54:58] <- paste(catNames[54], baseNames[54], as.character(inventory[3,][54:58]), sep="-") # Topic Filters - Main Topics
# baseNames[59:64] <- paste(catNames[54], baseNames[59], as.character(inventory[3,][59:64]), sep="-") # Topic Filters - Climate
# baseNames[65:70] <- paste(catNames[54], baseNames[65], as.character(inventory[3,][65:70]), sep="-") # Topic Filters - Ecosystems
# baseNames[71:78] <- paste(catNames[54], baseNames[71], as.character(inventory[3,][71:78]), sep="-") # Topic Filters - Agriculture / Built Environment
# baseNames[79:82] <- paste(catNames[54], baseNames[79], as.character(inventory[3,][79:82]), sep="-") # Topic Filters - Society
# baseNames[83:90] <- paste(catNames[54], baseNames[83], as.character(inventory[3,][83:90]), sep="-") # Topic Filters - Water / Flooding
# baseNames[93:94] <- paste(catNames[93], baseNames[93:94], sep="-") # Contact Information
# baseNames[95:99] <- paste(catNames[93], baseNames[95:99], sep="-") # Contact Information
colnames(inventory) <- headers # baseNames
inventory <- inventory[which(inventory$`Tool ID`=="1"):nrow(inventory),] # Extract only the tools (Should remove the first 4 rows; the header rows)
invValsOnly <- inventory[which(is.na(inventory$`Group ID`)==F),] # Extract the only the ones with Group IDs - should be all the tools
##needed with tool id with new file, may not be needed in future versions
# Format the Tool ID to have just 1 decimal point and add a space padding if its a single digit number
invValsOnly$`Tool ID` <- as.character(format(round(as.numeric(invValsOnly$`Tool ID`),1)),nsmall=1)
# Remove all columns after: "Documentation-Related Publications"
doc_ind <- grep("Documentation", colnames(invValsOnly))
invValsOnly <- invValsOnly[ ,1:doc_ind[length(doc_ind)]]
# Remove columns that are crossed out in the excel sheet:
# "dat", "fig", "gis", "map", "rprt"
crossNames <- c("Tool Outputs-dat-dat- numerical data", "Tool Outputs-fig",
"Tool Outputs-gis", "Tool Outputs-map", "Tool Outputs-rprt-rprt- reports")
invValsOnly <- invValsOnly[ ,!(colnames(invValsOnly) %in% crossNames)]
# # Remove all column entries after `Contact Information-Related Publications`. These relate to creating the document
# # 39 is `Tool Outputs-rprt`
# # 45 is `Tool Outputs-Description`
# # > colnames(invValsOnly[40:44])
# # [1] "dat"  "fig"  "gis"  "map"  "rprt"
# # # hhh <- invValsOnly[,c(1:39, 45:99)]
# # Remove columns that are not useful including:
# # "dat", "fig", "gis", "map", "rprt"
# invValsOnly <- invValsOnly[,c(1:39, 45:99)]
##read in the template files to be editted
findTempFile <- list.files(dataDir, templatePageName, recursive=T, full.names=T)
templatePage <- paste(readLines(findTempFile), collapse="\n")
findIndFile <- list.files(dataDir, templateIndex, recursive=T, full.names=T)
templateIndexPg <- paste(readLines(findIndFile), collapse="\n")
findSearchTempFile <- list.files(dataDir, searchCodePage, recursive=T, full.names=T)
templateSearchTemp <- paste(readLines(findSearchTempFile), collapse="\n")
##split by group id, seems to be coorolated with developers
toolIDs <- unique(invValsOnly$`Tool ID`) # Make sure all values are different
toolIDs <- toolIDs[is.na(toolIDs)==F] # Make sure all values are not NA
# Convert the data.frame to a list where each item is a different tool
splitByToolID <- lapply(toolIDs, function(x){invValsOnly[which(invValsOnly$`Tool ID`==x & is.na(invValsOnly$`Tool ID`)==F),]})
##########################################################################
# Set up search engine??
##########################################################################
##create the various subdirectories for the site
if(dir.exists(toolsDir)==F){ # public/tools/ directory
dir.create(toolsDir, recursive=T)
}
if(dir.exists(discovDir)==F){ # public/data/ directory
dir.create(discovDir, recursive=T)
}
if(dir.exists(imageDir)==F){ # public/images/ directory
dir.create(imageDir, recursive=T)
}
# Create a series of empty data.frames with specific column names
# searchTags <- data.frame(toolName=NA, toolLink=NA, tag=NA)
# searchSoftReqs <- data.frame(toolName=NA, toolLink=NA, softReqs=NA)
# searchGeoScope <- data.frame(toolName=NA, toolLink=NA, toolState=NA, toolLoc=NA, coastal=NA)
# searchToolFunct <- data.frame(toolName=NA, toolLink=NA, toolFunct=NA)
# searchTopFilters <- data.frame(toolName=NA, toolLink=NA, topFilter=NA, topFilterNam=NA,  subFilter=NA)
# #searchSubFilters <- data.frame(toolName=NA, toolLink=NA, subFilter=NA)
#ttt <- data.frame(table(invValsOnly$`Geographic Scope-Scope`, useNA="ifany"))
#hhh <- invValsOnly[invValsOnly$`Geographic Scope-Scope`=="National",]
# Run the generateToolPage function on each tool. #### IT APPEARS THE ABOVE DATA.FRAMES ARE USED GLOBALLY!!
toolpage_list = pblapply(splitByToolID, generateToolPage, tempPage=templatePage,
el=el, siteDir=toolsDir, scTab=stcntyTab, updateContent=TRUE)
##libraries
library(readxl)
library(pbapply)
library(stringi)
# install.packages("webshot")
library(webshot)
library(openxlsx)
##webshot::install_phantomjs()
library(dplyr)
library(stringr)
#/Volumes/GoogleDrive/Shared drives/MARISA/Coastal Climate Extension Pilot/Tool Inventory/Tool Screenshots_resized_156_250/TOOLID_1.0_ScreenCapture-1.png
##find data file, assumed to be on computer
# baseDir <- "/Users/mdl5548/Documents/GitHub/"
# dataDir <- paste0(baseDir, "climToolsReference/siteGeneration/")
# #siteDir <- paste0(dataDir, "docs/")
# siteDir <- paste0(baseDir, "climToolsReference/content/")
# toolsDir <- paste0(siteDir, "tools_test/")
# discovDir <- paste0(baseDir, "climToolsReference/public/data_test/")
# imageDir <- paste0(baseDir, "climToolsReference/public/images_test/")
# toolSearchDir <- paste0(baseDir, "climToolsReference/themes/soho/layouts/page/")
baseDir <- "~/Documents/Github/"
dataDir <- paste0(baseDir, "climToolsReference/siteGeneration/")
source(paste0(dataDir, "siteGeneratingFunctions_2022.R"))
siteDir <- paste0(baseDir, "climToolsReference/content/")
toolsDir <- paste0(siteDir, "tools_test/")
discovDir <- paste0(baseDir, "climToolsReference/public/data_test/")
imageDir <- paste0(baseDir, "climToolsReference/public/images_test/")
#toolInventoryName <- "Inventory_2020-01-21_v2.xlsx"
toolInventoryName <- "Tool Inventory - 12.15.21 copy.xlsx"
templatePageName <- "pageTemplate.md"
templateIndex <- "templateIndex.md"
searchCodePage <- "searchTemplate.html"
stcntyFile <- paste0(dataDir, "MARISAstateFIPS.csv")
statefips = read.csv(paste0(dataDir, "stateFIPS.csv"))
##line break character
el <- "\n"
tab <- "  "
##read in the tool description document
toolInventoryFile <- list.files(dataDir, toolInventoryName, recursive=T, full.names=T)
toolInventoryFile <- toolInventoryFile[grep("~",toolInventoryFile, invert=TRUE)]
#toolInventoryFile <- descFile[grep(paste0("/",toolInventoryFile), descFile)]
##get number of sheets, to load all of them
sheetNames <- excel_sheets(toolInventoryFile)
##read in data
# Each list represents a seperate excel sheet
# toolInventory <- lapply(sheetNames, read_excel, path=toolInventoryFile, col_names=F)
# Use read.xlsx to get the merged cells
toolInventory <- lapply(sheetNames, read.xlsx, xlsxFile=toolInventoryFile, colNames=F, fillMergedCells = TRUE)
names(toolInventory) <- sheetNames
stcntyTab <- read.csv(stcntyFile) # Read the fips csv file
##inventory, cleaning headers to be useful
inventory <- toolInventory$Inventory # toolInventory[[1]]
inventory <- toolInventory$Inventory
catNames <- as.character(inventory[1,]) # Top line header (category names) for sheet 1: Inventory
baseNames <- as.character(inventory[2,]) # Secondary header (sub-category names) for sheet 1: Inventory
level3Names <- as.character(inventory[3,]) # Secondary header (sub-category names) for sheet 1: Inventory
level4Names <- as.character(inventory[4,]) # Secondary header (sub-category names) for sheet 1: Inventory
# Combine the top line and secondary headers to set as the column names
headers = rep(NA, length(catNames))
for(i in 1:length(catNames)){
if(is.na(catNames[i])){
headers[i] = baseNames[i]
} else {
headers[i] = paste(catNames[i], baseNames[i], sep="-")
}
}
# Find the duplicate headers and add the third category of names to the header
n_occur <- data.frame(table(headers))
dup_headers <- unique(headers[headers %in% n_occur$headers[n_occur$Freq > 1]])
ind_dup = unlist(lapply(X = dup_headers, function(X){which(headers == X)}))
headers[ind_dup] = paste(headers[ind_dup], level3Names[ind_dup], sep="-")
# Find the duplicate headers and add the third category of names to the header
n_occur_4level <- data.frame(table(headers))
dup_headers_4level <- unique(headers[headers %in% n_occur_4level$headers[n_occur_4level$Freq > 1]])
# Make sure website friendliness is included (it is currently not a duplicate)
if(!("Description-Tags-Website friendliness" %in% dup_headers_4level)){
dup_headers_4level <- c(dup_headers_4level, "Description-Tags-Website friendliness")
}
ind_dup_4level = unlist(lapply(X = dup_headers_4level, function(X){which(headers == X)}))
headers[ind_dup_4level] = paste(headers[ind_dup_4level], level4Names[ind_dup_4level], sep="-")
# # Combine the top line and secondary headers to set as the column names
# ##for now, hard coded
# baseNames[6:10] <- paste(catNames[6], baseNames[6:10], sep="-") # Description
# baseNames[11:15] <- paste(catNames[11], baseNames[11:15], sep="-") # Geographic Scope
# baseNames[16:24] <- paste(catNames[16], baseNames[16], as.character(inventory[3,][16:24]), sep="-") # Purpose - Tool Function
# baseNames[25:28] <- paste(catNames[16], baseNames[25], as.character(inventory[3,][25:28]), sep="-") # Purpose - Future Projections
# baseNames[29] <- paste(catNames[16], baseNames[29], sep="-") # Purpose - Description
# baseNames[30:34] <- paste(catNames[30], baseNames[30:34], sep="-") # Tool type
# baseNames[35:36] <- paste(catNames[35], baseNames[35:36], sep="-") # Tool Inputs
# baseNames[37:39] <- paste(catNames[37], baseNames[37:39], sep="-") # Tool Outputs
# baseNames[45] <- paste(catNames[37], baseNames[45], sep="-")# Tool Outputs
# baseNames[46:50] <- paste(catNames[46], baseNames[46:50], sep="-") # Software Requirements
# baseNames[51:53] <- paste(catNames[51], baseNames[51:53], sep="-") # Effort / Skill
# baseNames[54:58] <- paste(catNames[54], baseNames[54], as.character(inventory[3,][54:58]), sep="-") # Topic Filters - Main Topics
# baseNames[59:64] <- paste(catNames[54], baseNames[59], as.character(inventory[3,][59:64]), sep="-") # Topic Filters - Climate
# baseNames[65:70] <- paste(catNames[54], baseNames[65], as.character(inventory[3,][65:70]), sep="-") # Topic Filters - Ecosystems
# baseNames[71:78] <- paste(catNames[54], baseNames[71], as.character(inventory[3,][71:78]), sep="-") # Topic Filters - Agriculture / Built Environment
# baseNames[79:82] <- paste(catNames[54], baseNames[79], as.character(inventory[3,][79:82]), sep="-") # Topic Filters - Society
# baseNames[83:90] <- paste(catNames[54], baseNames[83], as.character(inventory[3,][83:90]), sep="-") # Topic Filters - Water / Flooding
# baseNames[93:94] <- paste(catNames[93], baseNames[93:94], sep="-") # Contact Information
# baseNames[95:99] <- paste(catNames[93], baseNames[95:99], sep="-") # Contact Information
colnames(inventory) <- headers # baseNames
inventory <- inventory[which(inventory$`Tool ID`=="1"):nrow(inventory),] # Extract only the tools (Should remove the first 4 rows; the header rows)
invValsOnly <- inventory[which(is.na(inventory$`Group ID`)==F),] # Extract the only the ones with Group IDs - should be all the tools
##needed with tool id with new file, may not be needed in future versions
# Format the Tool ID to have just 1 decimal point and add a space padding if its a single digit number
invValsOnly$`Tool ID` <- as.character(format(round(as.numeric(invValsOnly$`Tool ID`),1)),nsmall=1)
# Remove all columns after: "Documentation-Related Publications"
doc_ind <- grep("Documentation", colnames(invValsOnly))
invValsOnly <- invValsOnly[ ,1:doc_ind[length(doc_ind)]]
# Remove columns that are crossed out in the excel sheet:
# "dat", "fig", "gis", "map", "rprt"
crossNames <- c("Tool Outputs-dat-dat- numerical data", "Tool Outputs-fig",
"Tool Outputs-gis", "Tool Outputs-map", "Tool Outputs-rprt-rprt- reports")
invValsOnly <- invValsOnly[ ,!(colnames(invValsOnly) %in% crossNames)]
# # Remove all column entries after `Contact Information-Related Publications`. These relate to creating the document
# # 39 is `Tool Outputs-rprt`
# # 45 is `Tool Outputs-Description`
# # > colnames(invValsOnly[40:44])
# # [1] "dat"  "fig"  "gis"  "map"  "rprt"
# # # hhh <- invValsOnly[,c(1:39, 45:99)]
# # Remove columns that are not useful including:
# # "dat", "fig", "gis", "map", "rprt"
# invValsOnly <- invValsOnly[,c(1:39, 45:99)]
##read in the template files to be editted
findTempFile <- list.files(dataDir, templatePageName, recursive=T, full.names=T)
templatePage <- paste(readLines(findTempFile), collapse="\n")
findIndFile <- list.files(dataDir, templateIndex, recursive=T, full.names=T)
templateIndexPg <- paste(readLines(findIndFile), collapse="\n")
findSearchTempFile <- list.files(dataDir, searchCodePage, recursive=T, full.names=T)
templateSearchTemp <- paste(readLines(findSearchTempFile), collapse="\n")
##split by group id, seems to be coorolated with developers
toolIDs <- unique(invValsOnly$`Tool ID`) # Make sure all values are different
toolIDs <- toolIDs[is.na(toolIDs)==F] # Make sure all values are not NA
# Convert the data.frame to a list where each item is a different tool
splitByToolID <- lapply(toolIDs, function(x){invValsOnly[which(invValsOnly$`Tool ID`==x & is.na(invValsOnly$`Tool ID`)==F),]})
##########################################################################
# Set up search engine??
##########################################################################
##create the various subdirectories for the site
if(dir.exists(toolsDir)==F){ # public/tools/ directory
dir.create(toolsDir, recursive=T)
}
if(dir.exists(discovDir)==F){ # public/data/ directory
dir.create(discovDir, recursive=T)
}
if(dir.exists(imageDir)==F){ # public/images/ directory
dir.create(imageDir, recursive=T)
}
# Create a series of empty data.frames with specific column names
# searchTags <- data.frame(toolName=NA, toolLink=NA, tag=NA)
# searchSoftReqs <- data.frame(toolName=NA, toolLink=NA, softReqs=NA)
# searchGeoScope <- data.frame(toolName=NA, toolLink=NA, toolState=NA, toolLoc=NA, coastal=NA)
# searchToolFunct <- data.frame(toolName=NA, toolLink=NA, toolFunct=NA)
# searchTopFilters <- data.frame(toolName=NA, toolLink=NA, topFilter=NA, topFilterNam=NA,  subFilter=NA)
# #searchSubFilters <- data.frame(toolName=NA, toolLink=NA, subFilter=NA)
#ttt <- data.frame(table(invValsOnly$`Geographic Scope-Scope`, useNA="ifany"))
#hhh <- invValsOnly[invValsOnly$`Geographic Scope-Scope`=="National",]
# Run the generateToolPage function on each tool. #### IT APPEARS THE ABOVE DATA.FRAMES ARE USED GLOBALLY!!
toolpage_list = pblapply(splitByToolID, generateToolPage, tempPage=templatePage,
el=el, siteDir=toolsDir, scTab=stcntyTab, updateContent=TRUE)
# Tags
searchTags <- lapply(X = 1:length(toolpage_list), function(X){toolpage_list[[X]]$searchTags})
searchTags <- do.call(rbind.data.frame, searchTags)
# Software requirements
searchSoftReqs <- lapply(X = 1:length(toolpage_list), function(X){toolpage_list[[X]]$searchSoftReqs})
searchSoftReqs <- do.call(rbind.data.frame, searchSoftReqs)
# Geographic scope
searchGeoScope <- lapply(X = 1:length(toolpage_list), function(X){toolpage_list[[X]]$searchGeoScope})
searchGeoScope <- do.call(rbind.data.frame, searchGeoScope)
# Tool Function
searchToolFunct <- lapply(X = 1:length(toolpage_list), function(X){toolpage_list[[X]]$searchToolFunct})
searchToolFunct <- do.call(rbind.data.frame, searchToolFunct)
# Tp Filters
searchTopFilters <- lapply(X = 1:length(toolpage_list), function(X){toolpage_list[[X]]$searchTopFilters})
searchTopFilters <- do.call(rbind.data.frame, searchTopFilters)
# #
# Set non coastal areas to 0 and coastal areas to 1. Set to numeric values
searchGeoScope$coastal[is.na(searchGeoScope$coastal)] <- 0
searchGeoScope$coastal[searchGeoScope$coastal=="x"] <- 1
searchGeoScope$coastal <- as.numeric(searchGeoScope$coastal)
toolIndex <- gsub("TemplateIndex", "Tools", templateIndexPg)
toolIndex <- paste0(toolIndex, el, el, "A collection of articles, presentations or talks, most likely on stuff and stuff.")
writeFile <- paste0(toolsDir, "_index.md")
conFile <- file(writeFile)
writeLines(toolIndex, conFile)
close(conFile) # Make sure to close the connection to prevent errors or warnings
createRecs <- paste('{"softReq":"', searchSoftReqs$softReqs, '","name":"',
searchSoftReqs$toolName, '","link":"', searchSoftReqs$toolLink,
'"}', sep="", collapse=",\n")
softReqsData <- paste0('{', el, '"items":[', el, createRecs, el, ']', el, '}')
writeFile <- paste0(discovDir, "searchSoft.json")
conFile <- file(writeFile)
writeLines(softReqsData, conFile)
close(conFile) # Make sure to close the connection to prevent errors or warnings
createRecs <- paste('{"geoScopeST":"', searchGeoScope$toolState, '","name":"',
searchGeoScope$toolName, '","link":"', searchGeoScope$toolLink,
'"}', sep="", collapse=",\n")
geoScopeData <- paste0('{', el, '"items":[', el, createRecs, el, ']', el, '}')
writeFile <- paste0(discovDir, "searchGeoScope_state.json")
conFile <- file(writeFile)
writeLines(geoScopeData, conFile)
close(conFile) # Make sure to close the connection to prevent errors or warnings
##county
createRecs <- paste('{"geoScopeCNTY":"', searchGeoScope$toolLoc, '","name":"',
searchGeoScope$toolName, '","link":"', searchGeoScope$toolLink,
'"}', sep="", collapse=",\n")
geoScopeData <- paste0('{', el, '"items":[', el, createRecs, el, ']', el, '}')
writeFile <- paste0(discovDir, "searchGeoScope_county.json")
conFile <- file(writeFile)
writeLines(geoScopeData, conFile)
close(conFile) # Make sure to close the connection to prevent errors or warnings
fff <- cbind.data.frame(searchGeoScope$coastal, searchGeoScope$toolName, searchGeoScope$toolLink)
ggg <- unique(fff)
createRecs <- paste('{"geoScopeCoast":"', ggg$`searchGeoScope$coastal`, '","name":"',
ggg$`searchGeoScope$toolName`, '","link":"',
ggg$`searchGeoScope$toolLink`, '"}', sep="", collapse=",\n")
geoScopeData <- paste0('{', el, '"items":[', el, createRecs, el, ']', el, '}')
writeFile <- paste0(discovDir, "searchGeoScope_coastal.json")
conFile <- file(writeFile)
writeLines(geoScopeData, conFile)
close(conFile) # Make sure to close the connection to prevent errors or warnings
createRecs <- paste('{"toolFun":"', searchToolFunct$toolFunct, '","name":"',
searchToolFunct$toolName, '","link":"', searchToolFunct$toolLink,
'"}', sep="", collapse=",\n")
toolFunData <- paste0('{', el, '"items":[', el, createRecs, el, ']', el, '}')
writeFile <- paste0(discovDir, "searchToolFun.json")
conFile <- file(writeFile)
writeLines(toolFunData, conFile)
close(conFile) # Make sure to close the connection to prevent errors or warnings
createRecs <- paste('{"topFilter":"', searchTopFilters$topFilterNam, '","name":"',
searchTopFilters$toolName, '","link":"',
searchTopFilters$toolLink, '"}', sep="", collapse=",\n")
mainFiltData <- paste0('{', el, '"items":[', el, createRecs, el, ']', el, '}')
writeFile <- paste0(discovDir, "searchTopFilter.json")
conFile <- file(writeFile)
writeLines(mainFiltData, conFile)
close(conFile) # Make sure to close the connection to prevent errors or warnings
##create the tool search object jscript for sub-topic filter
createRecs <- paste('{"subFilter":"', searchTopFilters$subFilter, '","name":"',
searchTopFilters$toolName, '","link":"', searchTopFilters$toolLink,
'"}', sep="", collapse=",\n")
subFiltData <- paste0('{', el, '"items":[', el, createRecs, el, ']', el, '}')
writeFile <- paste0(discovDir, "searchSubFilter.json")
conFile <- file(writeFile)
writeLines(subFiltData, conFile)
close(conFile) # Make sure to close the connection to prevent errors or warnings
source("~/Documents/Github/climToolsReference/siteGeneration/generateSite_2022.R")
templateSearchTemp
findSearchTempFile
toolsDir
list.files("toolsDir", full.names = TRUE)
list.files(toolsDir, full.names = TRUE)
grep("page", list.files(toolsDir, full.names = TRUE))
grep("page", list.files(toolsDir, full.names = TRUE), value=TRUE)
file.create(file.path(
"/Users/klr324/Documents/Github/",
paste("test", 1:5, "txt", sep = ".")
))
dir(
"/Users/klr324/Documents/Github/",
pattern = "^test\\.[0-9]\\.txt$",
full.names = TRUE
)
file.remove(dir(
"/Users/klr324/Documents/Github/",
pattern = "^test\\.[0-9]\\.txt$",
full.names = TRUE
))
system("ls")
system("ls /Users/klr324/Documents/Github/test")
system("ls /Users/klr324/Documents/Github/test")
system("rm -rf /Users/klr324/Documents/Github/test/*")
paste0("rm -rf ", baseDir, "climToolsReference/public/tools/*")
baseDir
source("~/Documents/Github/climToolsReference/siteGeneration/generateSite_2022.R")
